{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18eee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pyautogui\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://comic.naver.com/webtoon'\n",
    "html = requests.get(URL).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "week = ['tab=mon', 'tab=tue', 'tab=wed', 'tab=thu', 'tab=fri', 'tab=sat', 'tab=sun', 'tab=finish']\n",
    "ko_week = ['월', '화', '수', '목', '금', '토', '일', '완결']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [] ; title_list = [] ; author_list = [] ; day_list = [] ; genre_list = [] ; \n",
    "story_list = [] ; platform_list = [] ; img_list = [] ; tag_list = [] ; \n",
    "grade_list = [] ; viewer_list = [] ; series_list = [] ; regist_list = [] ; href_lists = []\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome() # 크롬 사용하니까, 크롬 드라이버를 현재 파이썬 코드가 있는 폴더로!!\n",
    "\n",
    "def naver_login():\n",
    "    login = 'https://nid.naver.com/nidlogin.login?url=https%3A%2F%2Fcomic.naver.com%2Fwebtoon%3Ftab%3Dmon'\n",
    "    driver.implicitly_wait(2)\n",
    "    driver.maximize_window()\n",
    "    driver.get(login)\n",
    "    \n",
    "#     driver.find_element_by_css_selector(\"#id_email_2_label\").click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"id\"]').click()\n",
    "    pyperclip.copy('{아이디}')\n",
    "    pyautogui.hotkey('ctrl','v')\n",
    "    sleep(5)\n",
    "    \n",
    "#     driver.find_element_by_css_selector('#id_password_3_label').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"pw\"]').click()\n",
    "    pyperclip.copy('{비번}')\n",
    "    pyautogui.hotkey('ctrl','v')\n",
    "    sleep(5)\n",
    "\n",
    "    #로그인 버튼 \n",
    "#     login_send = browser.find_element_by_css_selector(\"#login-form > fieldset > div.wrap_btn > button.btn_g.btn_confirm.submit\")\n",
    "    login_send = driver.find_element_by_xpath('//*[@id=\"log.login\"]')\n",
    "    login_send.click()\n",
    "#     driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c44479",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(URL)\n",
    "driver.implicitly_wait(3) #로딩이 끝날 때까지 10초까지는 기다려줌\n",
    "naver_login() #로그인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    sleep(0.5)\n",
    "    uri = URL + \"?\" + week[i]\n",
    "    \n",
    "    driver.get(uri) #요일별로 링크 가져옴\n",
    "\n",
    "    html = requests.get(URL).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    sleep(0.5)\n",
    "\n",
    "    #스크롤 전 높이\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "    #무한 스크롤\n",
    "    while True:\n",
    "        #맨 아래로 스크롤을 내린다\n",
    "        driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "\n",
    "        #스크롤 사이 페이지 로딩 시간\n",
    "        sleep(1)\n",
    "\n",
    "        #스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "\n",
    "        before_h = after_h\n",
    "        \n",
    "    ## 웹툰 디테일 링크 + 별점\n",
    "\n",
    "    href_list = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/ul/li/a')\n",
    "    grades = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/ul/li/div/div/span')\n",
    "#     authors_li = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/ul/li/div/div/a')\n",
    "    \n",
    "    page_list = []\n",
    "    for page in href_list:\n",
    "        page_list.append(page.get_attribute('href'))\n",
    "\n",
    "    scores = []\n",
    "    for grade in grades:\n",
    "        scores.append(grade.text.replace(\"별점\", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "#     authors = []\n",
    "#     for author in authors_li:\n",
    "#         authors.append(author.text)\n",
    "        \n",
    "#     print(len(authors))\n",
    "#     for i in range(len(authors))\n",
    "#         print(authors[i])\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    for href in page_list :\n",
    "#     href = href_list[i].get_attribute('href')\n",
    "#     print(href_list[i].get_attribute('href'))\n",
    "#     print(grades[i].text.replace(\"별점\", \"\").replace(\"\\n\", \"\"))\n",
    "#     print(authors[i].text)\n",
    "\n",
    "        try:\n",
    "            author = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/ul/li[' + str(idx+1) + ']/div/div[1]/a')\n",
    "        except:\n",
    "            author = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/ul/li['+ str(idx+1) + ']/div/a[2]')\n",
    "        print(author.text)\n",
    "        author_info = author.text\n",
    "        \n",
    "        sleep(2)\n",
    "\n",
    "        driver.get(href + \"&page=1&sort=ASC\") # 디테일 이동\n",
    "    \n",
    "        html = requests.get(href).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "        sleep(2)\n",
    "    \n",
    "        day = ko_week[i]\n",
    "    \n",
    "        ## 제목\n",
    "        title = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/h2').text\n",
    "        if (title in title_list):  # 요일 두 개 이상이면 요일만 추가함\n",
    "                    day_list[title_list.index(title)] += ', ' + day\n",
    "                    driver.back()\n",
    "                    continue\n",
    "                \n",
    "#         print(title)\n",
    "        title_list.append(title)\n",
    "    \n",
    "        grade_list.append(scores[idx])\n",
    "    \n",
    "        id_list.append(num) ; num += 1  # id 리스트에 추가\n",
    "        href_lists.append(href)\n",
    "        \n",
    "        first = driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/ul/li[1]/a/div[2]/div/span[2]').text\n",
    "        print(first)\n",
    "        regist_list.append(\"20\" + first)\n",
    "        \n",
    "        ## 이미지\n",
    "        thumb = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/button/div/img').get_attribute('src')\n",
    "#         print(thumb)\n",
    "        img_list.append(thumb)\n",
    "        \n",
    "        ## 저자\n",
    "#         print(authors[i])\n",
    "#         author_list.append(authors[idx])\n",
    "        author_list.append(author_info)\n",
    "    \n",
    "        ## 요일\n",
    "#         print(day)\n",
    "        day_list.append(day)\n",
    "    \n",
    "        ## 줄거리\n",
    "        story = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/p').text\n",
    "#         print(story)\n",
    "        story_list.append(story)\n",
    "    \n",
    "        ## 장르 & 태그\n",
    "        tags = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/div/a')\n",
    "        temp = \"\"\n",
    "        genre = tags[0].text.replace(\"#\", \"\")\n",
    "        genre_list.append(genre)\n",
    "        \n",
    "        for tag in tags:\n",
    "#             print(tag.text)\n",
    "            temp += tag.text.replace(\"#\", \"\") + \" \"\n",
    "        tag_list.append(temp)\n",
    "        \n",
    "        ## 관심 수\n",
    "        viewer = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div/button[1]/span[2]').text\n",
    "#         print(viewer)\n",
    "        viewer_list.append(viewer)\n",
    "    \n",
    "        ## 총 회차\n",
    "        try:\n",
    "            series = driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/div[1]/div[1]').text\n",
    "        except:\n",
    "            series = driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/div[2]/div[1]').text\n",
    "            #         print(series)\n",
    "        series = series.replace(\"총 \", \"\")\n",
    "        series = series.replace(\"화\", \"\")\n",
    "        series_list.append(series)\n",
    "        \n",
    "        platform_list.append(\"네이버\")\n",
    "    \n",
    "        driver.back()\n",
    "        \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.DataFrame()\n",
    "total_data['id'] = id_list\n",
    "total_data['title'] = title_list\n",
    "total_data['img'] = img_list ## 썸네일\n",
    "total_data['author'] = author_list\n",
    "total_data['day'] = day_list\n",
    "total_data['genre'] = genre_list\n",
    "total_data['story'] = story_list\n",
    "total_data['tag'] = tag_list\n",
    "total_data['viewer'] = viewer_list\n",
    "total_data['grade'] = grade_list\n",
    "total_data['regist'] = regist_list\n",
    "total_data['series'] = series_list\n",
    "total_data['href'] = href_lists\n",
    "total_data['platform'] = platform_list\n",
    "total_data.to_csv('new_href_네이버_웹툰2.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72275d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 완결 ######\n",
    "driver.get('https://comic.naver.com/webtoon?tab=finish') #요일별로 링크 가져옴\n",
    "\n",
    "html = requests.get(URL).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "sleep(0.5)\n",
    "\n",
    "#스크롤 전 높이\n",
    "before_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "#무한 스크롤\n",
    "while True:\n",
    "    #맨 아래로 스크롤을 내린다\n",
    "    driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "\n",
    "    #스크롤 사이 페이지 로딩 시간\n",
    "    sleep(1)\n",
    "\n",
    "    #스크롤 후 높이\n",
    "    after_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "    if after_h == before_h:\n",
    "        break\n",
    "\n",
    "    before_h = after_h\n",
    "        \n",
    "## 웹툰 디테일 링크 + 별점\n",
    "\n",
    "href_list = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/ul/li/a')\n",
    "grades = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/ul/li/div/div/span')\n",
    "#     authors_li = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/ul/li/div/div/a')\n",
    "    \n",
    "page_list = []\n",
    "for page in href_list:\n",
    "    page_list.append(page.get_attribute('href'))\n",
    "\n",
    "scores = []\n",
    "for grade in grades:\n",
    "    scores.append(grade.text.replace(\"별점\", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "#     authors = []\n",
    "#     for author in authors_li:\n",
    "#         authors.append(author.text)\n",
    "        \n",
    "#     print(len(authors))\n",
    "#     for i in range(len(authors))\n",
    "#         print(authors[i])\n",
    "    \n",
    "idx = 0\n",
    "\n",
    "for href in page_list :\n",
    "#     href = href_list[i].get_attribute('href')\n",
    "#     print(href_list[i].get_attribute('href'))\n",
    "#     print(grades[i].text.replace(\"별점\", \"\").replace(\"\\n\", \"\"))\n",
    "#     print(authors[i].text)\n",
    "\n",
    "    #스크롤 전 높이\n",
    "    before_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "    #무한 스크롤\n",
    "    while True:\n",
    "    #맨 아래로 스크롤을 내린다\n",
    "        driver.find_element_by_css_selector(\"body\").send_keys(Keys.END)\n",
    "\n",
    "    #스크롤 사이 페이지 로딩 시간\n",
    "        sleep(1)\n",
    "\n",
    "        #스크롤 후 높이\n",
    "        after_h = driver.execute_script(\"return window.scrollY\")\n",
    "\n",
    "        if after_h == before_h:\n",
    "            break\n",
    "\n",
    "        before_h = after_h\n",
    "\n",
    "    try:\n",
    "        author = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/ul/li[' + str(idx+1) + ']/div/div[1]/a')\n",
    "    except:\n",
    "        try:\n",
    "            author = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/ul/li['+ str(idx+1) + ']/div/a[2]')\n",
    "        except:\n",
    "            try:\n",
    "                author = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/ul/li[' + str(idx+1) + ']/div/div[1]/a')\n",
    "            except:\n",
    "                print(\"#########지나갑니다\")\n",
    "                continue\n",
    "                \n",
    "    print(author.text)\n",
    "    author_info = author.text\n",
    "        \n",
    "    sleep(2)\n",
    "\n",
    "    driver.get(href + \"&page=1&sort=ASC\") # 디테일 이동\n",
    "    \n",
    "    html = requests.get(href).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    day = ko_week[i]\n",
    "    \n",
    "        ## 제목\n",
    "    title = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/h2').text\n",
    "    if (title in title_list):  # 요일 두 개 이상이면 요일만 추가함\n",
    "                day_list[title_list.index(title)] += ', ' + day\n",
    "                driver.back()\n",
    "                continue\n",
    "                \n",
    "#         print(title)\n",
    "    title_list.append(title)\n",
    "    \n",
    "    grade_list.append(scores[idx])\n",
    "    \n",
    "    id_list.append(num) ; num += 1  # id 리스트에 추가\n",
    "        \n",
    "    first = driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/ul/li[1]/a/div[2]/div/span[2]').text\n",
    "    print(first)\n",
    "    regist_list.append(\"20\" + first)\n",
    "    \n",
    "    ## 이미지\n",
    "    thumb = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/button/div/img').get_attribute('src')\n",
    "#         print(thumb)\n",
    "    img_list.append(thumb)\n",
    "        \n",
    "    ## 저자\n",
    "#         print(authors[i])\n",
    "#         author_list.append(authors[idx])\n",
    "    author_list.append(author_info)\n",
    "    \n",
    "    ## 요일\n",
    "#         print(day)\n",
    "    day_list.append(day)\n",
    "    \n",
    "    ## 줄거리\n",
    "    story = driver.find_element_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/p').text\n",
    "#         print(story)\n",
    "    story_list.append(story)\n",
    "    \n",
    "    ## 장르 & 태그\n",
    "    tags = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[1]/div/div[2]/div/div/a')\n",
    "    temp = \"\"\n",
    "    genre = tags[0].text.replace(\"#\", \"\")\n",
    "    genre_list.append(genre)\n",
    "        \n",
    "    for tag in tags:\n",
    "#             print(tag.text)\n",
    "        temp += tag.text.replace(\"#\", \"\") + \" \"\n",
    "    tag_list.append(temp)\n",
    "        \n",
    "        ## 관심 수\n",
    "    viewer = driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div/button[1]/span[2]').text\n",
    "#         print(viewer)\n",
    "    viewer_list.append(viewer)\n",
    "    \n",
    "    ## 총 회차\n",
    "    try:\n",
    "        series = driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/div[1]/div[1]').text\n",
    "    except:\n",
    "        series = driver.find_element_by_xpath('//*[@id=\"content\"]/div[3]/div[2]/div[1]').text\n",
    "            #         print(series)\n",
    "    series = series.replace(\"총 \", \"\")\n",
    "    series = series.replace(\"화\", \"\")\n",
    "    series_list.append(series)\n",
    "        \n",
    "    platform_list.append(\"네이버\")\n",
    "    \n",
    "    driver.back()\n",
    "        \n",
    "    idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
